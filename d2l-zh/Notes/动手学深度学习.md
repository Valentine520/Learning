# 动手学深度学习

## 引言

这是学习的内容和路线 

![image-20231025081341546](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231025081341546.png)

![image-20231025082320157](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231025082320157.png)
官方文档要求 对于 Map-style 的Dataset 这里有 `__getitem__()` `__len__()` 需要overwrite 告诉iter到底该如何获得数据 

并且还可以使用并行化版本 有点难度

![image-20231025082309945](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231025082309945.png)



深度学习就是用来解决数据缺失的问题 在训练过程中为了训练过程的可靠性 避免crash 可以对数据进行插值 和 删除 

利用`torch.fillna()` 来进行插值 处理缺失值问题

还可以将那些非数值问题转换为数值来分析 例如 address 是一种地址 然后可以将这些文字转换为一些index 对于对应的index就对应了相应的address 

这样处理之后所有数据都包含在tensor里

![image-20231025084806706](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231025084806706.png)



矩阵乘法是比较重要的 分为`numpy.matmul` 以及 另外一种`numpy.multiply` 前者是完全的矩阵乘法 后者是在向量化过程中会经常用到的 权重和input之间的乘积 后者还可以直接使用 `*` 进行乘法 



对于`np.sum` 如果你的matrix dimension 是[2, 3, 4] 你可以指定sum的方式 求和哪个维度 哪个维度就消失 例如 `np.sum(axis=0)` 那么最后得到的就是 [3, 4] 的二维矩阵 得到的是两个大块矩阵求和的结果



安装GPU 版本的深度学习框架是存在一定难度的 需要对应好各项软件的配置 还是比纯纯手写快多了

![image-20231025091700323](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231025091700323.png)



`keepdims` 在sum的过程中可以保持维度 方便我们进行广播 以及后续的运算 



在机器学习过程中，通常使用训练误差和测试误差进行比较 我们希望降低训练误差 让我们的模型能够更好进行拟合 解决(underfitting)问题 同时也要缩短测试误差和训练误差之间的差距 解决(overfitting) 问题

这一切都是建立在一种test  和 train 是独立同分布的基础上 但是往往训练内容和测试内容 或者应用内容还是存在一定差别的 例如测试的图像不一定每一张都是清晰的 在严苛的环境下 模型的表现可能就不尽人意



对于超参数来说 本身都不应该在训练集中进行训练 例如 对于多元线性回归来说 如果多项式的次数是一个超参数 在训练集中该超参数永远都是越大越好 导致最终必然的overfitting

如果想要训练一些超参数 例如learning rate 可以引入验证集 对于测试集来说 只是用来评估模型训练后的泛化能力 是对人而言的 并不能给模型任何直接的修改意见 例如学习率过大 要减少一些 所以validation set 的存在是有意义的

**K折交叉验证算法** 

模型的评估结果对于测试集和训练集的划分是有关的 如果你的集合比较小 就无法完全评估模型在所有数据集上的平均误差 也就是性能

所以可以将数据集划分为2个模块 一个是training 一个是 test 然后每次随机抽取一个test作为测试集合 然后计算这几次处理之后的均值

![img](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imagev2-dd15830a7231489187e607182efd376e_1440w.webp)



对于一般的机器学习来说 大多都是给出一些低维内容 然后将内容拟合或者记录下来 就好像你知道了答案 然后让你去回答一个新的问题

但是如果你的数据维度过高 对于一些复杂问题 例如图像识别 语音识别 这些问题可能就会涉及到一个巨大的维度空间 如果没有对应的数据点 传统的ML 算法就没有办法给出对应的回答

所以需要引入深度学习 这是一种解决高维难题的方法



计算图可以将求导的链式过程表示为一个图 然后在正向传播的过程中就计算对应的梯度 然后save 反向传播时就可以直接计算

![image-20231030080039339](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030080039339.png)

正向是我们最常用的形式 从小到大 正向 但是反向则是从最小的到最终的

![image-20231030080351912](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030080351912.png)

反向过程需要大量的中间结果 也就是正向传播的结果 反向传播有些节点按需计算 

![image-20231030080456978](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030080456978.png)

时间和空间复杂度分析 因为要保留梯度 所以深度学习所耗费的内存数量是惊人的 因为我们要对每一层都计算梯度 所以这种在正向传播保留梯度的方法 是优于在反向传播过程中实时计算梯度的 大大减少神经网络的训练时间

![image-20231030080550687](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030080550687.png)



感知机 和 多层感知机

感知机很简单 和线性回归很相似 但是最后有一个activation function 是 non-linear的

![image-20231030084309790](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030084309790.png)

线性回归是一个实数 这里我们输出的是一个类别

早些年间的感知机训练 我们只有两个类别 如果你的y_hat 是一个正数 但是y 实际上是一个负数 那么就会导致小于0 分类错误  这里是乘法 异号则代表预测错误 

如果分类错误 则更新w 和 b，方法也很简单粗暴，等价于batch为1的梯度下降 

可以用一个单一的损失函数来定义 max(0, -y<w, x>) 如果预测成功 则当前loss为0 梯度也为0 不更新 如果预测失败则为正数 需要更新参数来降低损失函数

![image-20231030084339680](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030084339680.png)

不断地更新参数 然后所有的分类正确之后就可以停止 但是很难收敛 要求所有的类都能满足

![image-20231030084925584](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030084925584.png)

**感知机的收敛和类别之间的空隙有关 如果在一个很小的空间内 算法对任何类的分别都是正确的 那么就可以在该空间内训练出一个感知机** 

但是如果不存在这个空间 无论怎样训练都无法保证100%的正确 或者说类别之间存在交叉 感知机的训练可能是无法收敛的

![image-20231030085047094](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030085047094.png)



### XOR问题

感知机无法拟合这种问题

线性模型最大的特点就是只有单一的一条线 如果使用简单的感知机 画一条线 是无法将所有类别都区分开的 所以这就是线性模型在处理复杂问题中的局限 我们需要非线性激活函数

感知机是最早的AI模型，同时因为不能拟合XOR问题，引发了机器学习的一个寒冬

为了解决这种局限 我们需要使用多层感知机

![image-20231030085342245](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030085342245.png)





### 多层感知机

上述的类别的区分至少需要两条线 所以我们可以先确定一条线 例如纵线 分割左右两侧 然后再来一根横线 这样我们就利用两根线切成了不同的方格

利用多个感知机组合 我们可以将XOR问题拆分为多个小问题 例如分类左右两侧 或者上下两侧 组合之后就可以得到对整个问题的分类

![image-20231030085737171](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030085737171.png)

这里不同的线就是一个感知机 组合成模型来说 这就是多层感知机 一个layer中含有多个分类器 

如果一次做不了 我们可以分别学习多个特征 创造多个函数

![image-20231030085931800](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030085931800.png)

这和现今的CNN是密不可分的 卷积神经网络也是先学会如何识别线 然后再学习如何识别面 然后再学习如何识别3d 轮廓等 将复杂问题分解为一个又一个的卷积层来处理 和直接识别一个整体的图像相比 更简单也更强大

单隐藏层的多层感知机 隐藏层的大小也是一个超参数 设计一个较好的矩阵

![image-20231030090136895](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030090136895.png)

上述多层感知机可以用类似单层的方法进行计算 只是这里引入了一个 activation function 用来获取非线性的特性

![image-20231030090248035](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030090248035.png)



激活函数也有很多种 例如 ReLU Sigmoid tanh等 非线性激活函数保证各项的拟合能够提取出不同的特征

ReLU 计算起来很方便 没有指数运算 求导起来也很方便 所以经常使用

Softmax 是一种线性激活函数 通常用在最后一层 用来处理最后的结果 操作实际上是将一系列value 归一化之后映射到一个概率上 保证相加都为1 这样操作我们就可以将value转为概率 来评估一个分类模型

网络的结构你可以自己设计 多个hidden layer 也是可以的 多个输出也是可以的 但是注意维度的匹配

**输出是不需要激活函数的 激活函数就是用来避免我们的layer 坍塌 避免坍缩成为一个纯线性模型**

**但是在输出的最后一层 我们可以使用线性激活函数 例如softmax 来将我们的输出转为一个概率值**



如果你的任务很复杂 数据量很大 你可以使用一个很深的网络 或者在single hidden layer model中expand 这个单一隐藏层的维度 令其变得越复杂越好 **你的网络可以很胖或者很高 很高的容易训练 所以是深度学习** 一口吃个胖子 很难过拟合 

这和人来学习是很相似的 先学数字 再学加减 再学导数 积分 所以是由简单到复杂的 不可能直接就学导数 所以深度学习是很方便训练的 

如果是较深的网络 你需要不断精简你的网络 提取信息 所以你的维度应该是不断下降 但是你的features 或者 channels的数量应该要不断上升



应用多层感知机 和 单纯的softmax回归 实际上相差得并不是很多 深度学习在一定程度上也保证了输出变化的稳定性 尽管你改进了一些奇怪的东西 但是你的结果不会有特别离谱的变化 这也增加了优化的难度



通常一层神经网络指的是线性映射 以及非线性激活 为整体一层 但是在nn中实现起来 却要分开 



神经网络的参数决定了神经网络的框架和结构 所以需要调参 80%的专家都是在不断调参





## 模型的选择和过(欠)拟合



### 模型的验证

可能你的训练数据集就是这么巧合 有很多穿蓝色衣服的人 所以你会认为穿蓝色衣服就是你的模型的特征 但是这并不能作为特征 因为你的数据集分布有问题 实际上有很多红色衣服的人也加入其中 只是训练数据并没有捕捉到 模型的泛化能力很差 

![image-20231030114453703](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030114453703.png)

所以我们需要利用训练误差、泛化误差 来衡量你的模型存在的问题 到底是过拟合还是欠拟合 采取对应的措施

这里就涉及到一些专门用于模型eval的数据集 验证集 validation dataset 以及 测试集 testing dataset 

不要将validation dataset 和 training set 混在一起

![image-20231030114931144](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030114931144.png)



如果你用训练过的数据拿来作验证集 你的模型可能会虚高 你根本不清楚问题所在 看上去一切都很优秀 但就是表现得不好

**测试数据集也只会使用一次 完全用来评估效果 不要用测试数据集来修改参数 而要用验证数据集**

要完全遵守训练过程的规则 否则会出现很严重的错误 即你的模型只是在你这里表现得很好 但是真实应用起来就很差 不要自欺欺人

但是很多时候你的数据集数据量不够（现在大数据应该不存在这样的问题） 所以要切分出训练集 测试集 和 验证集 是一件比较困难的事 所以你可以使用**K-则交叉验证** 

算法比较简单 选择小部分的一个作为验证数据 然后其余的作为测试数据 然后计算平均值即可 这样就弥补了数据不足问题

![image-20231030115740545](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030115740545.png)

**k-折交叉验证 用多次validation的验证来平均掉因为数据量不足产生的误差** 





### 过拟合和欠拟合

模型的容量和数据可以用来评估到底是过拟合还是欠拟合 如果一个简单数据你都可以模拟得特别好 好过头了 那么必然是overfitting 如果你的数据集很大 但是你容量很低 必然训练程度还是不足

![image-20231030141506932](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030141506932.png)

对于过拟合和欠拟合来说，欠拟合，模型啥都学不到，无法代表你的数据特征；过拟合，模型将所有的细节都学下来了，你的训练数据被过度理解了

![image-20231030141459185](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030141459185.png)

这里的图像很好解释了欠拟合和过拟合对模型的影响 如果模型容量不断增加 过于关注训练集的细节 那么对于测试集来说 必然会增大

深度学习的核心是 首先你的模型足够复杂 足够大 然后我们再利用一定的手段降低模型的容量 减小泛化误差 解决过拟合问题 所以过拟合问题是首要的

![image-20231030141838930](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030141838930.png)

#### 估计模型容量

**模型的容量要匹配数据复杂度**

对于一些不同类型的模型 是很难比较复杂度的 例如树和神经网络

给定一个模型 有两个最主要的影响因素 一个是参数的个数 一个是参数值的选择范围

你可以完全用参数个数 来粗略估计模型的复杂度 例如一个有3 billion parameters的模型和一个仅仅3 million parameters的模型 还是能够轻松看出差别的

![image-20231030143603429](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030143603429.png)



数理统计提出一种VC维的概念 但是深度学习很少使用数理统计中的一些概念 也许是因为深度学习的本身难解释性导致的



#### 数据复杂度

可以利用样本个数 每个样本的元素个数 时间、空间结构 多样性 来衡量



对于过拟合现象 我们可以利用一个简单的线性回归模型来衡量 例如测试集和训练集完全不够的一个训练情景 下图2 所以反复训练出的模型是不够格的 会导致欠拟合 数据不够 如果测试loss 反而上升 就代表模型过拟合了



![image-20231030152051790](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030152051790.png)



![image-20231030152056541](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030152056541.png)

![image-20231030152224170](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030152224170.png)







## 权重衰退

上述提到的过拟合问题 可以使用权重衰退的方法来解决 这也就是正则化

当权重过大时 也很容易导致过拟合问题？ 难以训练 所以需要对权重的整体值进行一个限定 可以使用权重的均方误差 来进行限制 这就是 regularization

![image-20231030195051945](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030195051945.png)

这里我们不会限制bias 不会产生太大影响 更小的 $\theta$ 意味着更强的正则化

$\lambda$ 作为一种超参数来设置这种限制 越大表示限制更加强 需要极小的权重

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030195342952.png" alt="image-20231030195342952" style="zoom:80%;" />

当引入权重的大小作为惩罚项时 可能会导致原本的最优解为了权衡权重的值 而反而上升 形成一个平衡 **这样就减少了模型的复杂度 解决了过拟合问题**

![image-20231030195545022](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030195545022.png)

添加了这一项 然后计算梯度的过程中 一定会导致w的减少 相比于没有这个项 w的更新越来越小 每一步更新都缩小当前权重 然后再进行梯度下降更新

![image-20231030195801581](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030195801581.png)

因为神经网络总是会产生一定程度的overfitting 所以研究正则化是重要的

如果不进行权重衰减的正则化 最后训练的结果会导致严重的overfitting

![image-20231030201625995](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030201625995.png)

使用了正则化 你可以看到最后的w 是一个比较小的矩阵 并且test和train之间的差距也变小了 这样一定程度减少了overfitting

![image-20231030201715569](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030201715569.png)

如果使用api来处理 框架为每种优化器都提供了一个 weight decay 的选项 就像此处的SGD

![image-20231030201908513](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030201908513.png)



#### 为什么减小weight 就减小了模型的复杂度？

实际上我们减小weight 是对该模型做出了一个范围的假设 就是让该模型在一个很平稳的区间内去虚取参数值 而不像之前一样总是在很大很小的值中进行取值 这就会导致模型的及其不规则 所以模型的复杂度就高 很难保证不overfitting

所以减小模型的weight实际上是在为该模型设置一个选择weight的区间 让数据集可以以一种更加平滑的方式进行选择 这样就减少了模型的复杂性 就解决了overfitting问题

实际过程中的wd 也就是weight decay值选择 e-3 e-4 0.01 0.001 等 选项并不多 合适就行 但是效果不是特别好 只有一点点效果 还有其他类型的正则化方法可以用来优化

同时因为数据存在噪音 所以添加正则化项将其拉回到正规 能够有效改进模型 这也是正则化的有效性的另一种说明





## Dropout

如果你的数据存在噪音 那么相当于一定程度的正则 因为你需要适应对应的带有噪音的数据 所以你的模型不能过拟合 所以训练出的模型具有较强的鲁棒性和泛化性

但还可以在层之间加入一些噪音 也就是可以选择性地删掉或者屏蔽一些层 这就是**dropout 正则化方法**

此外 我们还希望添加一些噪音之后 整个模型的均值和方差 保持不变 所以我们将有概率地将某些unit屏蔽 并且同时除以一个1-p 来扩大对应地unit 保证整个模型的均值和方差保持不变

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030223904150.png" alt="image-20231030223904150" style="zoom:80%;" />

因为全连接层中比较方便去掉某些中间节点 并且对最后的结果并没有很严重的影响 只要不完全清除即可 （有没有一种运气特别差的极端情况，你的中间节点全都被清除了？概率很小 几乎不会发生）

因为overfitting就是模型容量过高了 模型对于训练数据的拟合过于复杂了 导致整个模型才会过拟合 所以如果清除掉某些unit 就变相减少了layer的结构 简化了某一层的结构

![image-20231030224342756](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030224342756.png)

正则化操作只是在训练过程中避免对训练数据的过拟合 而在validation 或者 testing 阶段并不会进行正则化操作

每次激活的都是一些子神经网络 

在实验过程中 dropout 操作表现出来的特性很像一个正则项 所以我们通常将其视为一个正则操作 但理论来说是不对的 不过确实可以这样用 实践才是唯一正确的

**Summary**

丢弃概率通常设置为0.5 反正不要设置为1 这样会导致整个网络呈现一种病态结构

![image-20231030225442515](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030225442515.png)

利用不同的参数简单测试对应的dropout_layer 函数 可以发现参数为0 或者 1 的极端值都会让输出结果特殊化

所以我们需要一些中间值来简化我们的结构 折中

![image-20231030230521150](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030230521150.png)



使用dropout操作要注意一点 我们对training 才会使用该函数 如果不是training阶段 则不会进行正则化

所以在创建网络的时候我们设置了一个is_training 参数作为判断 判断是否需要使用dropout正则化

![image-20231030235412805](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231030235412805.png)



当拥有了dropout之后 你可以将你的模型设置得非常大 然后之后利用正则化方法来解决过拟合问题

dropout实际上每次都取出一个子网络 对其他置零的网络实际上是没有任何影响的 也就是之分析这个子网络



神经网络是很难重复的 包括参数的初始化 参数的随机优化 cuda的随机加速 等 所以神经网络的正确性和可重复性是很难保证的

dropout是给全连接层用的 **BN(Batch Normalization)** 是给卷积层用的 wd 均适用 并且用得十分广泛

dropout 很方便调参 所以也非常常用 但是wd很难理解 不够直观

dropout 只每一次取出一个子网 所以收敛可能变慢 







## 数值稳定性

### Gradient exploded and vanishing

神经网络如果比较深 那么很容易就导致数值的不稳定性 如果对一个网络进行gradient descent 那么就涉及到大量的矩阵乘法 



![image-20231031095153896](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031095153896.png)

太多的矩阵乘法会导致 **梯度爆炸和梯度消失**两个问题 python中的数值表示总有一个上限 如果数字实在太大 则直接为 无穷 如果实在太小 则为0 会一直持续

![image-20231031095254978](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031095254978.png)

例如：在一个MLP中

![image-20231031095433451](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031095433451.png)

如果网络很深 这种类型的矩阵乘法要做很多次 如果W的值恰好也是有一些大的 这就会导致 gradient exploded

同时也存在 gradient vanishing

![image-20231031095606040](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031095606040.png)



#### Gradient exploded issues:

机器学习通常使用float16 作为dtype 所以很容易就超出了该类型的上限 导致overflow 这也很严重

同时也对学习率很敏感 因为w的值很大 如果大一点 则会导致更严重的gradient exploded 如果小一点 可能会因为学习率太小 训练没有任何变化

![image-20231031095734388](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031095734388.png)

#### Gradient vanishing issues：

同样有浮点数类型的最低精度问题 没有能力保存那么小的数字 并且训练没有任何进展 

backward propagation is from up to bottom, so the bottom layers will getting worse.

没有办法让更深的网络适用 你的深度神经网络实际上也是一个比较浅的网络 也就意味着 神经网络没有办法继续变深了

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031100057039.png" alt="image-20231031100057039" style="zoom:80%;" />

还有一类 如果在编程中计算loss或者一些值 可能会导致 /0 这里需要添加一个很小的bias 项 保证不会出现这种错误 





### Make training more stable

为了解决上述的问题 例如LSTM 是个时序很长的神经网络 那么它会采取某些措施 将其中的乘法变成加法 这就一定程度上解决了梯度爆炸和梯度消失问题 

或者将梯度归一化 不论有多大 都给你拉回来 

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031100509378.png" alt="image-20231031100509378" style="zoom:80%;" />

最容易做到的也就是合理的权重初始化和激活函数的选择 

**合理的权重初始和激活函数可以提高数值稳定性**

我们的目的就是通过权重和激活函数来控制每一层的输入和梯度 让它们都保持在一个可以接受的范围内 并且保持一定的分布特性

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031100753219.png" alt="image-20231031100753219" style="zoom:80%;" />

所以我们需要在合理的区间内初始化我们的初始参数 因为训练开始的时候很陡 梯度很大 更容易有数值不稳定问题 在最优解的附近表面会比较平 

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031100946213.png" alt="image-20231031100946213" style="zoom:80%;" />

如何来实现呢？ **考虑一个MLP的例子**

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031101226606.png" alt="image-20231031101226606" style="zoom:80%;" />

没有激活函数的输出后 方差的计算：

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031101331593.png" alt="image-20231031101331593" style="zoom:80%;" />



如果要保证输入和输出的方差保持一致 那么就需要那个项为1 反向亦是如此 

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031101437254.png" alt="image-20231031101437254" style="zoom:80%;" />

所以就导致要满足 ntyt = 1 and nt-1yt = 1，如果要完全满足该条件，是做不到的，除非输入维度刚好等于输出维度 但是我们可以选择一个较为折中的办法

#### Xavier 随机初始化方法

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031101619585.png" alt="image-20231031101619585" style="zoom: 80%;" />

根据输入层和输出层进行权重初始化适配 这种方法可以做到自适应 根据输入和输出的维度来调整 更加灵活合理

**同时在初始化时可以选择不同的分布来初始化参数矩阵为不同的值 这和我们只知道使用一个random值来初始化参数是有所提升的**

似乎深度学习的每个环节都可以进行优化提高 数据集的选择 权重的初始化 迭代过程中的规则 优化器等



上述的Xavier 初始化是在没有激活函数的背景下考虑的 如果我们添加一个线性激活函数（虽然我们不常用），可以发现如果要保证方差和均值的一致性 我们使用的线性激活函数必须是一个等于自身的函数 （也就是只添加bias，实际上还是自身，等于不添加激活函数）

<img src="https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031102206429.png" alt="image-20231031102206429" style="zoom:80%;" />



根据上述的思路，我们可以检查一些常用的激活函数，将其泰勒展开，我们可以发现其实许多函数都是近似于自身的 包括ReLU tanh 但是 sigmoid函数展开后比较奇怪 

但我们也可以很方便地使用一些线性变化来调整sigmoid函数 即 4 * sigmoid(x) - 2 将其方差和均值控制在一定的范围内

换一种思考方式 为何tanh和relu是优秀的激活函数 也可以从维持数值稳定性方面给出一定的回答

![image-20231031102439044](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031102439044.png)





### QA

如果训练一开始有效 但是过了一段时间保持在50% 并且很长时间不动 很有可能就是发生了数值稳定性问题 要么是gradient vanishing 要么是 exploded 
所以我们要使用一些已有的知名网络 LSTM ResNet等 来**缓解**这种数值不稳定问题 但数值不稳定问题会经常存在
整个深度学习都是在解决这种数值不稳定问题



如果在训练过程中发现了一些nan 数值 很有可能是发生了 gradient exploded



尽管深度学习对数学要求不高 但是仍然要能够看论文 推导 所以复杂的数学公式仍然要掌握 决定了上限 数学不好的时候 很多东西都看不懂



fp16 因为比较短小 可以提高训练速度 和 fp32 fp64相比 虽然很容易发生梯度爆炸和梯度消失问题 但是内存和速度的收益可以将两种潜在的可以避免的缺陷抵消 甚至fp64也会发生梯度的问题 如果不加以干预



梯度爆炸和消失 都不是由激活函数产生的 激活函数都比较平滑



**重点：**

如果强制让数值保持在一定的区间 比如之前是[0-1000] 你将其缩小为[0-10] 尽管数值是变小了 但都是同比例缩放 只是在计算机视角内变小了 更方便计算了 但是之前的特征仍然不会改变 就像一张图片 **你可以随时放大缩小 那张图片仍然还是那张图片 特性不会变化 所以不会改变均值和方差是不会改变数据的特征的 也不会降低模型的准确性和表达能力**





## Kaggle 竞赛：预测房价

在dataset中定义 `__len__` 函数是必要的 方便DataLoader 为你创建batch的数据 同时你可以自己len(xx) 来访问数据的大小 手工切片

![image-20231031161956675](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031161956675.png)



这个函数和其另外一个int的index版本不一样 data.iloc[xx] 可以直接索引列 但是对于data.loc[row, columns] 则必须先索引行 之后再索引列

![image-20231031171701490](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031171701490.png)



此外 如果数据是使用csv文件加载的 那么我们需要将数据转换为tensor才能使用深度学习的training method

![image-20231031172234797](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031172234797.png)

直接在 `__getitem__` 函数中实现这些内容

![image-20231031172328087](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031172328087.png)



这种错误是因为尽管我们在之前声明了values 作为global variable 但是在局部作用域中 这种values 并不会透析到外面 而是在局部中新建一个和之前无关的variable 

![image-20231031185541424](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/imageimage-20231031185541424.png)



tmux 用法

https://www.ruanyifeng.com/blog/2019/10/tmux.html



在进行数据处理的时候 可以使用这种方法来处理 让那些为numeric的部分保持方差为1 均值为0 然后填充0 即可

![截屏2023-11-03 09.27.33](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/%E6%88%AA%E5%B1%8F2023-11-03%2009.27.33.png)

其次 对于房价而言 不可以直接使用金额作为衡量loss的标准 因为小房子和大房子本身就存在数额上的差别 所以应该使用百分比 

即 $(y - y\_hat)/y$ 

![image-20231103093101266](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103093101266.png)

在进行训练等操作时进行合适的数据预处理 也是很关键的 思考如何来衡量loss 否则训练任务无法达到target

![](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103093101266.png)



如果数据量不够的话 可以使用K-折交叉验证

![image-20231103101925741](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103101925741.png)

每次折的时候都计算loss 然后计算平均

![截屏2023-11-03 10.20.02](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/%E6%88%AA%E5%B1%8F2023-11-03%2010.20.02.png)

对于这个任务 模型的使用是比较简单的 但是仍然要进行参数调优 选择合理的超参数 并且要进行合适的非线性激活





## 神经网络基础(Pytorch)

在神经网络的定义中 torch给我们一个方便的 `nn.Module` 你要注意的是 定义初始化神经网络层的时候 不可以使用嵌套 但是可以使用sequential

例如 `self.hidden = nn.ReLU(nn.Linear(10, 20))` 是不行的 但是forwad中却可以如此操作

`nn.Sequential(nn.Linear, nn.ReLU)` 是一个非常特殊的类 可以让我们将想要的layer都添加其中构成一个整体的模型 然后可以直接创建实例 很方便地帮助我们构建一个神经网络

![image-20231103102742367](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103102742367.png)

如果你定义了一个weight 然后你关闭了它的grad 那么这个变量就视为一种constant 不参加梯度运算和更新

你可以在forward中手写这个weight的计算过程 我们可以做更灵活的方法 你想如何前向计算都可以

不过前向就已经定义好了反向 所以你无需关心



![image-20231103103418317](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103103418317.png)



当然 你可以使用多个网络进行配合 将其写入 `nn.Sequential` 中进行搭配使用即可 你可以很方便增添或者修改你的模型 但总之 都要继承 `nn.Module` 并且注意结构

![image-20231103103639714](https://hacker-oss-typora.oss-cn-chengdu.aliyuncs.com/Learning/image-20231103103639714.png)

















